{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "030d95bc-2f9d-492b-8245-b791b866936b",
   "metadata": {},
   "source": [
    "---\n",
    "title: Migrating from ConversationalChain\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d20aeaad-b3ca-4a7d-b02d-3267503965af",
   "metadata": {},
   "source": [
    "[`ConversationChain`](https://api.python.langchain.com/en/latest/chains/langchain.chains.conversation.base.ConversationChain.html) incorporates a memory of previous messages to sustain a stateful conversation.\n",
    "\n",
    "Some advantages of switching to the LCEL implementation are:\n",
    "\n",
    "- Innate support for threads/separate sessions. To make this work with `ConversationChain`, you'd need to instantiate a separate memory class outside the chain.\n",
    "- More explicit parameters. `ConversationChain` contains a hidden default prompt, which can cause confusion.\n",
    "- Streaming support. `ConversationChain` only supports streaming via callbacks.\n",
    "\n",
    "`RunnableWithMessageHistory` implements sessions via configuration parameters. It should be instantiated with a callable that returns a [chat message history](https://api.python.langchain.com/en/latest/chat_history/langchain_core.chat_history.BaseChatMessageHistory.html). By default, it expects this function to take a single argument `session_id`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99b47ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade --quiet langchain langchain-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "717c8673",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00df631d-5121-4918-94aa-b88acce9b769",
   "metadata": {},
   "source": [
    "## Legacy\n",
    "\n",
    "<details open>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f2cc6dc-d70a-4c13-9258-452f14290da6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'how are you?',\n",
       " 'history': '',\n",
       " 'response': \"Arr matey, I be doin' well on the high seas, plunderin' and pillagin' as usual. How be ye?\"}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "template = \"\"\"\n",
    "You are a pirate. Answer the following questions as best you can.\n",
    "Chat history: {history}\n",
    "Question: {input}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "memory = ConversationBufferMemory()\n",
    "\n",
    "chain = ConversationChain(\n",
    "    llm=ChatOpenAI(),\n",
    "    memory=memory,\n",
    "    prompt=prompt,\n",
    ")\n",
    "\n",
    "chain({\"input\": \"how are you?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e36b0e-c7dc-4130-a51b-189d4b756c7f",
   "metadata": {},
   "source": [
    "</details>\n",
    "\n",
    "## LCEL\n",
    "\n",
    "<details open>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "666c92a0-b555-4418-a465-6490c1b92570",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Arr, me matey! I be doin' well, sailin' the high seas and searchin' for treasure. How be ye?\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a pirate. Answer the following questions as best you can.\"),\n",
    "        (\"placeholder\", \"{chat_history}\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "history = InMemoryChatMessageHistory()\n",
    "\n",
    "\n",
    "def get_history():\n",
    "    return history\n",
    "\n",
    "\n",
    "chain = prompt | ChatOpenAI() | StrOutputParser()\n",
    "\n",
    "wrapped_chain = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_history,\n",
    "    history_messages_key=\"chat_history\",\n",
    ")\n",
    "\n",
    "wrapped_chain.invoke({\"input\": \"how are you?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b386ce6-895e-442c-88f3-7bec0ab9f401",
   "metadata": {},
   "source": [
    "The above example uses the same `history` for all sessions. The example below shows how to use a different chat history for each session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96152263-98d7-4e06-8c73-d0c0abf3e8e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ahoy there, me hearty! What can this old pirate do for ye today?'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "store = {}\n",
    "\n",
    "\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = InMemoryChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "\n",
    "chain = prompt | ChatOpenAI() | StrOutputParser()\n",
    "\n",
    "wrapped_chain = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history,\n",
    "    history_messages_key=\"chat_history\",\n",
    ")\n",
    "\n",
    "wrapped_chain.invoke(\n",
    "    {\"input\": \"Hello!\"},\n",
    "    config={\"configurable\": {\"session_id\": \"abc123\"}},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2717810",
   "metadata": {},
   "source": [
    "</details>\n",
    "\n",
    "## Next steps\n",
    "\n",
    "See [this tutorial](/docs/tutorials/chatbot) for a more end-to-end guide on building with [`RunnableWithMessageHistory`](https://api.python.langchain.com/en/latest/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html).\n",
    "\n",
    "Check out the [LCEL conceptual docs](/docs/concepts/#langchain-expression-language-lcel) for more background information."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
